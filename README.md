## How the Algorithm Works

This project implements a genetic algorithm to locate the global minimum of a stochastic objective function using population-based evolutionary optimization.

### 1. Objective Function Generation
A stochastic objective function is generated by simulating a noisy random process, smoothing it with a rolling mean, and transforming it into a one-dimensional array. Each index represents a candidate solution, and the corresponding value represents the quantity to be minimized.

### 2. Population Initialization
An initial population is created by randomly sampling indices from the objective function. Each individual corresponds to a possible solution within the search space.

### 3. Fitness Evaluation
Fitness is computed as the negative value of the objective function at each individual’s index, converting the minimization task into a maximization problem suitable for evolutionary optimization.

### 4. Parent Selection
Parents are selected using fitness-proportional (roulette-wheel) selection. Individuals with better fitness have a higher probability of being selected, while weaker individuals remain to preserve genetic diversity.

### 5. Crossover
Selected parents undergo crossover using a weighted blending strategy controlled by the parameter α. This combines information from both parents to generate new candidate solutions.

### 6. Mutation
Offspring may undergo mutation, where their indices are randomly shifted within bounded limits. Mutation promotes exploration of the search space and helps prevent premature convergence.

### 7. Population Update and Convergence
The offspring replace the previous population while maintaining a fixed population size. This evolutionary process is repeated for a predefined number of generations. During training, the algorithm prints the best fitness value at each generation:

Generation X: Best Fitness = ...

This output shows the population steadily improving over time. By the final generation, the population typically contains indices that are very close to or exactly at the true global minimum of the objective function.

### 8. Best Solution Identification
After the final generation, the algorithm identifies the best solution found and reports:

- the index of the best solution  
- the objective function value at that index  

Example output:
Best solution found at index 412 with value 0.031

This represents the algorithm’s final estimate of the minimum.

### 9. Validation Against the True Minimum
Because the true global minimum is known and computed using `np.min(f)` and `np.argmin(f)`, the algorithm explicitly validates its result.

**Exact success case:**
Success: The algorithm found the global minimum.

**Near-optimal case:**
The algorithm did not find the global minimum.

Even in the near-optimal case, the solution is typically very close to the true minimum
